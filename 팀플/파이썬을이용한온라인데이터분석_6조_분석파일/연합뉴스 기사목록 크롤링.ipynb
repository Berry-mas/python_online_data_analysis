{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c9d9bd9-21f8-455a-9c37-e617ab292e9e",
   "metadata": {},
   "source": [
    "# 1. 연합뉴스"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0775f718-280a-4a00-8492-c494dfc2f6f0",
   "metadata": {},
   "source": [
    "# 1.(1) 검색어: '윤석열'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "93f307e1-27ba-4ecc-a779-469106e38da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터가 CSV 파일로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 첫페이지\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Chrome 옵션 설정\n",
    "chrome_options = Options()\n",
    "chrome_options.add_experimental_option(\"detach\", True)\n",
    "chrome_options.add_argument(\"--disable-notifications\")  # 알림 비활성화\n",
    "chrome_options.add_argument(\"--start-maximized\")  # 브라우저 최대화\n",
    "chrome_options.add_argument(\"--disable-popup-blocking\")  # 팝업 차단 비활성화\n",
    "\n",
    "# ChromeDriver 경로 설정 및 Service 객체 생성\n",
    "s = Service(r\"C:\\Users\\junse\\2024-2\\python_onlinedata_analysis\\chromedriver-win64\\chromedriver.exe\")  # ChromeDriver 실행 파일 경로\n",
    "driver = webdriver.Chrome(service=s, options=chrome_options)  # WebDriver 객체 생성\n",
    "\n",
    "# 연합뉴스 사이트 접속\n",
    "url_Y = \"https://www.yna.co.kr/\"\n",
    "driver.get(url_Y)\n",
    "\n",
    "try:\n",
    "    # 돋보기\n",
    "    search_icon =  WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, '/html/body/header/div[1]/span[3]/button[2]/i[1]'))\n",
    "    ) \n",
    "    search_icon.click()\n",
    "    \n",
    "    # 검색창 대기\n",
    "    search_input = WebDriverWait(driver, 20).until(\n",
    "        EC.presence_of_element_located((By.XPATH, '//*[@id=\"liveSrch01\"]'))\n",
    "    )\n",
    "    # 검색어 입력\n",
    "    search_input.send_keys(\"윤석열\")\n",
    "    search_input.send_keys(Keys.RETURN)  # 엔터키로 검색\n",
    "\n",
    "    # 다음 페이지 로딩 대기\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.XPATH, '//*[@id=\"search_detail_inbox\"]/fieldset/div[1]/div[1]/label'))\n",
    "    )\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"검색창에 접근할 수 없습니다:\", e) \n",
    "    driver.quit()\n",
    "    exit()\n",
    "    \n",
    "# 검색 기간 설정\n",
    "try:\n",
    "    # 직접입력 누르기\n",
    "    direct_input = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, '//*[@id=\"perioddirect\"]'))\n",
    "    )\n",
    "    direct_input.click()           \n",
    "\n",
    "    #정확도 누르기\n",
    "    accuracy = driver.find_element(By.XPATH, '//*[@id=\"srchAlign02\"]')\n",
    "    accuracy.click()\n",
    "    \n",
    "    # 검색 시작 날짜\n",
    "    start_date = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, '//*[@id=\"from\"]'))\n",
    "    )\n",
    "    start_date.send_keys(\"20220215\")\n",
    "    # 검색 종료 날짜\n",
    "    end_date = driver.find_element(By.XPATH, '//*[@id=\"to\"]')\n",
    "    end_date.click()\n",
    "    end_date.send_keys(\"20220308\") \n",
    "\n",
    "    # 검색 누르기\n",
    "    search_icon = driver.find_element(By.XPATH, '//*[@id=\"search_click\"]')\n",
    "    search_icon.click()\n",
    "\n",
    "    # 다음 페이지 로딩 대기\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.XPATH, '//*[@id=\"article_list\"]/div[2]/ul/li[1]/a/span[1]'))\n",
    "    )\n",
    "    \n",
    "    # 더보기 누르기 \n",
    "    more_articles = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, '//*[@id=\"article_list\"]/div[1]/span[2]/a'))\n",
    "    )\n",
    "    more_articles.click()\n",
    "    \n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"검색 기간을 설정할 수 없습니다:\", e)\n",
    "    driver.quit()\n",
    "    exit()\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "# 기사 데이터 크롤링\n",
    "time.sleep(5)\n",
    "titles, links, dates = [], [], [] # 제목, 링크, 작성일자를 저장할 리스트 생성\n",
    "# 페이지 HTML 가져오기\n",
    "page_source = driver.page_source\n",
    "soup = BeautifulSoup(page_source, \"lxml\")  # BeautifulSoup으로 HTML 파싱\n",
    "\n",
    "# <div class=\"cts_atclst\"> 내부의 모든 <li> 태그 찾기\n",
    "div_tag = soup.find(\"div\", class_=\"cts_atclst\")\n",
    "if div_tag:\n",
    "    articles = div_tag.find_all(\"li\")  # <li> 태그 리스트 반환\n",
    "    for article in articles:\n",
    "        try:\n",
    "            # 제목 추출\n",
    "            title_tag = article.find(\"span\", class_=\"tt2\")\n",
    "            title = title_tag.get_text(strip=True) if title_tag else \"제목 없음\"\n",
    "            titles.append(title)\n",
    "\n",
    "            # 링크 추출\n",
    "            link_tag = article.find(\"a\")\n",
    "            link = link_tag[\"href\"] if link_tag and link_tag.has_attr(\"href\") else \"링크 없음\"\n",
    "            links.append(link)\n",
    "\n",
    "            # 작성일자 추출\n",
    "            date_tag = article.find(\"span\", class_=\"pbdt\")\n",
    "            date = date_tag.get_text(strip=True) if date_tag else \"날짜 없음\"\n",
    "            dates.append(date)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"개별 기사 처리 중 오류 발생: {e}\")\n",
    "     \n",
    "#--------------------------------------------------------------------------------------------------------------\n",
    "# 데이터 저장\n",
    "try:\n",
    "    df = pd.DataFrame({\"제목\": titles, \"링크\": links, \"작성일자\": dates})\n",
    "    df = df.drop_duplicates()\n",
    "    df.to_csv(\"윤석열_연합뉴스_기사목록.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    print(\"데이터가 CSV 파일로 저장되었습니다.\")\n",
    "except Exception as e:\n",
    "    print(\"CSV 저장 중 오류 발생:\", e)\n",
    "\n",
    "# 브라우저 종료\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "df49fe50-807a-4726-a91e-9e6762f765f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터가 CSV 파일로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "## 이거 반복\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Chrome 옵션 설정\n",
    "chrome_options = Options()\n",
    "chrome_options.add_experimental_option(\"detach\", True)\n",
    "chrome_options.add_argument(\"--disable-notifications\")  # 알림 비활성화\n",
    "chrome_options.add_argument(\"--start-maximized\")  # 브라우저 최대화\n",
    "chrome_options.add_argument(\"--disable-popup-blocking\")  # 팝업 차단 비활성화\n",
    "\n",
    "# ChromeDriver 경로 설정 및 Service 객체 생성\n",
    "s = Service(r\"C:\\Users\\junse\\2024-2\\python_onlinedata_analysis\\chromedriver-win64\\chromedriver.exe\")  # ChromeDriver 실행 파일 경로\n",
    "driver = webdriver.Chrome(service=s, options=chrome_options)  # WebDriver 객체 생성\n",
    "\n",
    "# 연합뉴스 사이트 접속\n",
    "for numb in range(2,51):\n",
    "    url_Y = f\"https://www.yna.co.kr/search/index?query=%EC%9C%A4%EC%84%9D%EC%97%B4&ctype=A&sort=weight&from=20220215&to=20220308&period=diy&page_no={numb}\"\n",
    "    driver.get(url_Y)\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "    # 기사 데이터 크롤링\n",
    "    time.sleep(5)\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, \"lxml\")  # BeautifulSoup으로 HTML 파싱\n",
    "\n",
    "    # <div class=\"cts_atclst\"> 내부의 모든 <li> 태그 찾기\n",
    "    div_tag = soup.find(\"div\", class_=\"cts_atclst\")\n",
    "    if div_tag:\n",
    "        articles = div_tag.find_all(\"li\")  # <li> 태그 리스트 반환\n",
    "        for article in articles:\n",
    "            try:\n",
    "                # 제목 추출\n",
    "                title_tag = article.find(\"span\", class_=\"tt2\")\n",
    "                title = title_tag.get_text(strip=True) if title_tag else \"제목 없음\"\n",
    "                titles.append(title)\n",
    "\n",
    "                # 링크 추출\n",
    "                link_tag = article.find(\"a\")\n",
    "                link = link_tag[\"href\"] if link_tag and link_tag.has_attr(\"href\") else \"링크 없음\"\n",
    "                links.append(link)\n",
    "\n",
    "                # 작성일자 추출\n",
    "                date_tag = article.find(\"span\", class_=\"pbdt\")\n",
    "                date = date_tag.get_text(strip=True) if date_tag else \"날짜 없음\"\n",
    "                dates.append(date)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"개별 기사 처리 중 오류 발생: {e}\")\n",
    "#--------------------------------------------------------------------------------------------------------------\n",
    "# 데이터 저장\n",
    "try:\n",
    "    df = pd.DataFrame({\"제목\": titles, \"링크\": links, \"작성일자\": dates})\n",
    "    df.to_csv(\"윤석열_연합뉴스_기사목록.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    print(\"데이터가 CSV 파일로 저장되었습니다.\")\n",
    "except Exception as e:\n",
    "    print(\"CSV 저장 중 오류 발생:\", e)\n",
    "\n",
    "# 브라우저 종료\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7575904b-952b-467e-beb7-a2b2ab074c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터가 CSV 파일로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "## 그냥 이거 하나만\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Chrome 옵션 설정\n",
    "chrome_options = Options()\n",
    "chrome_options.add_experimental_option(\"detach\", True)\n",
    "chrome_options.add_argument(\"--disable-notifications\")  # 알림 비활성화\n",
    "chrome_options.add_argument(\"--start-maximized\")  # 브라우저 최대화\n",
    "chrome_options.add_argument(\"--disable-popup-blocking\")  # 팝업 차단 비활성화\n",
    "\n",
    "# ChromeDriver 경로 설정 및 Service 객체 생성\n",
    "s = Service(r\"C:\\Users\\junse\\2024-2\\python_onlinedata_analysis\\chromedriver-win64\\chromedriver.exe\")  # ChromeDriver 실행 파일 경로\n",
    "driver = webdriver.Chrome(service=s, options=chrome_options)  # WebDriver 객체 생성\n",
    "titles, links, dates = [], [], [] # 제목, 링크, 작성일자를 저장할 리스트 생성\n",
    "\n",
    "# 연합뉴스 사이트 접속\n",
    "for numb in range(1,21):\n",
    "    url_Y = f\"https://www.yna.co.kr/search/index?query=%EC%9C%A4%EC%84%9D%EC%97%B4&ctype=A&sort=weight&from=20220215&to=20220308&period=diy&page_no={numb}\"\n",
    "    driver.get(url_Y)\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "    # 기사 데이터 크롤링\n",
    "    time.sleep(5)\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, \"lxml\")  # BeautifulSoup으로 HTML 파싱\n",
    "\n",
    "    # <div class=\"cts_atclst\"> 내부의 모든 <li> 태그 찾기\n",
    "    div_tag = soup.find(\"div\", class_=\"cts_atclst\")\n",
    "    if div_tag:\n",
    "        articles = div_tag.find_all(\"li\")  # <li> 태그 리스트 반환\n",
    "        for article in articles:\n",
    "            try:\n",
    "                # 제목 추출\n",
    "                title_tag = article.find(\"span\", class_=\"tt2\")\n",
    "                title = title_tag.get_text(strip=True) if title_tag else \"제목 없음\"\n",
    "                titles.append(title)\n",
    "\n",
    "                # 링크 추출\n",
    "                link_tag = article.find(\"a\")\n",
    "                link = link_tag[\"href\"] if link_tag and link_tag.has_attr(\"href\") else \"링크 없음\"\n",
    "                links.append(link)\n",
    "\n",
    "                # 작성일자 추출\n",
    "                date_tag = article.find(\"span\", class_=\"pbdt\")\n",
    "                date = date_tag.get_text(strip=True) if date_tag else \"날짜 없음\"\n",
    "                dates.append(date)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"개별 기사 처리 중 오류 발생: {e}\")\n",
    "#--------------------------------------------------------------------------------------------------------------\n",
    "# 데이터 저장\n",
    "try:\n",
    "    df = pd.DataFrame({\"제목\": titles, \"링크\": links, \"작성일자\": dates})\n",
    "    df.to_csv(\"윤석열_연합뉴스_기사목록200.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    print(\"데이터가 CSV 파일로 저장되었습니다.\")\n",
    "except Exception as e:\n",
    "    print(\"CSV 저장 중 오류 발생:\", e)\n",
    "\n",
    "# 브라우저 종료\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c4f46d-fa2c-4fec-9165-017e35bcf5b4",
   "metadata": {},
   "source": [
    "# 1.(2) 검색어: '이재명'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "52f99549-ca53-4874-ba7a-a7aae651e41c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터가 CSV 파일로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 첫페이지\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Chrome 옵션 설정\n",
    "chrome_options = Options()\n",
    "chrome_options.add_experimental_option(\"detach\", True)\n",
    "chrome_options.add_argument(\"--disable-notifications\")  # 알림 비활성화\n",
    "chrome_options.add_argument(\"--start-maximized\")  # 브라우저 최대화\n",
    "chrome_options.add_argument(\"--disable-popup-blocking\")  # 팝업 차단 비활성화\n",
    "\n",
    "# ChromeDriver 경로 설정 및 Service 객체 생성\n",
    "s = Service(r\"C:\\Users\\junse\\2024-2\\python_onlinedata_analysis\\chromedriver-win64\\chromedriver.exe\")  # ChromeDriver 실행 파일 경로\n",
    "driver = webdriver.Chrome(service=s, options=chrome_options)  # WebDriver 객체 생성\n",
    "\n",
    "# 연합뉴스 사이트 접속\n",
    "url_Y = \"https://www.yna.co.kr/\"\n",
    "driver.get(url_Y)\n",
    "\n",
    "try:\n",
    "    # 돋보기\n",
    "    search_icon =  WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, '/html/body/header/div[1]/span[3]/button[2]/i[1]'))\n",
    "    ) \n",
    "    search_icon.click()\n",
    "    \n",
    "    # 검색창 대기\n",
    "    search_input = WebDriverWait(driver, 20).until(\n",
    "        EC.presence_of_element_located((By.XPATH, '//*[@id=\"liveSrch01\"]'))\n",
    "    )\n",
    "    # 검색어 입력\n",
    "    search_input.send_keys(\"이재명\")\n",
    "    search_input.send_keys(Keys.RETURN)  # 엔터키로 검색\n",
    "\n",
    "    # 다음 페이지 로딩 대기\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.XPATH, '//*[@id=\"search_detail_inbox\"]/fieldset/div[1]/div[1]/label'))\n",
    "    )\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"검색창에 접근할 수 없습니다:\", e) \n",
    "    driver.quit()\n",
    "    exit()\n",
    "    \n",
    "# 검색 기간 설정\n",
    "try:\n",
    "    # 직접입력 누르기\n",
    "    direct_input = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, '//*[@id=\"perioddirect\"]'))\n",
    "    )\n",
    "    direct_input.click()           \n",
    "\n",
    "    #정확도 누르기\n",
    "    accuracy = driver.find_element(By.XPATH, '//*[@id=\"srchAlign02\"]')\n",
    "    accuracy.click()\n",
    "    \n",
    "    # 검색 시작 날짜\n",
    "    start_date = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, '//*[@id=\"from\"]'))\n",
    "    )\n",
    "    start_date.send_keys(\"20220215\")\n",
    "    # 검색 종료 날짜\n",
    "    end_date = driver.find_element(By.XPATH, '//*[@id=\"to\"]')\n",
    "    end_date.click()\n",
    "    end_date.send_keys(\"20220308\") \n",
    "\n",
    "    # 검색 누르기\n",
    "    search_icon = driver.find_element(By.XPATH, '//*[@id=\"search_click\"]')\n",
    "    search_icon.click()\n",
    "\n",
    "    # 다음 페이지 로딩 대기\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.XPATH, '//*[@id=\"article_list\"]/div[2]/ul/li[1]/a/span[1]'))\n",
    "    )\n",
    "    \n",
    "    # 더보기 누르기 \n",
    "    more_articles = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, '//*[@id=\"article_list\"]/div[1]/span[2]/a'))\n",
    "    )\n",
    "    more_articles.click()\n",
    "    \n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"검색 기간을 설정할 수 없습니다:\", e)\n",
    "    driver.quit()\n",
    "    exit()\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "# 기사 데이터 크롤링\n",
    "time.sleep(5)\n",
    "titles, links, dates = [], [], [] # 제목, 링크, 작성일자를 저장할 리스트 생성\n",
    "# 페이지 HTML 가져오기\n",
    "page_source = driver.page_source\n",
    "soup = BeautifulSoup(page_source, \"lxml\")  # BeautifulSoup으로 HTML 파싱\n",
    "\n",
    "# <div class=\"cts_atclst\"> 내부의 모든 <li> 태그 찾기\n",
    "div_tag = soup.find(\"div\", class_=\"cts_atclst\")\n",
    "if div_tag:\n",
    "    articles = div_tag.find_all(\"li\")  # <li> 태그 리스트 반환\n",
    "    for article in articles:\n",
    "        try:\n",
    "            # 제목 추출\n",
    "            title_tag = article.find(\"span\", class_=\"tt2\")\n",
    "            title = title_tag.get_text(strip=True) if title_tag else \"제목 없음\"\n",
    "            titles.append(title)\n",
    "\n",
    "            # 링크 추출\n",
    "            link_tag = article.find(\"a\")\n",
    "            link = link_tag[\"href\"] if link_tag and link_tag.has_attr(\"href\") else \"링크 없음\"\n",
    "            links.append(link)\n",
    "\n",
    "            # 작성일자 추출\n",
    "            date_tag = article.find(\"span\", class_=\"pbdt\")\n",
    "            date = date_tag.get_text(strip=True) if date_tag else \"날짜 없음\"\n",
    "            dates.append(date)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"개별 기사 처리 중 오류 발생: {e}\")\n",
    "     \n",
    "#--------------------------------------------------------------------------------------------------------------\n",
    "# 데이터 저장\n",
    "try:\n",
    "    df = pd.DataFrame({\"제목\": titles, \"링크\": links, \"작성일자\": dates})\n",
    "    df = df.drop_duplicates()\n",
    "    df.to_csv(\"이재명_연합뉴스_기사목록.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    print(\"데이터가 CSV 파일로 저장되었습니다.\")\n",
    "except Exception as e:\n",
    "    print(\"CSV 저장 중 오류 발생:\", e)\n",
    "\n",
    "# 브라우저 종료\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "168578ca-8054-463b-a450-de165090d0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터가 CSV 파일로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "## 이거 반복\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Chrome 옵션 설정\n",
    "chrome_options = Options()\n",
    "chrome_options.add_experimental_option(\"detach\", True)\n",
    "chrome_options.add_argument(\"--disable-notifications\")  # 알림 비활성화\n",
    "chrome_options.add_argument(\"--start-maximized\")  # 브라우저 최대화\n",
    "chrome_options.add_argument(\"--disable-popup-blocking\")  # 팝업 차단 비활성화\n",
    "\n",
    "# ChromeDriver 경로 설정 및 Service 객체 생성\n",
    "s = Service(r\"C:\\Users\\junse\\2024-2\\python_onlinedata_analysis\\chromedriver-win64\\chromedriver.exe\")  # ChromeDriver 실행 파일 경로\n",
    "driver = webdriver.Chrome(service=s, options=chrome_options)  # WebDriver 객체 생성\n",
    "\n",
    "# 연합뉴스 사이트 접속\n",
    "for numb in range(2,51):\n",
    "    url_Y = f\"https://www.yna.co.kr/search/index?query=%EC%9D%B4%EC%9E%AC%EB%AA%85&ctype=A&sort=weight&from=20220215&to=20220308&period=diy&page_no={numb}\"\n",
    "    driver.get(url_Y)\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "    # 기사 데이터 크롤링\n",
    "    time.sleep(5)\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, \"lxml\")  # BeautifulSoup으로 HTML 파싱\n",
    "\n",
    "    # <div class=\"cts_atclst\"> 내부의 모든 <li> 태그 찾기\n",
    "    div_tag = soup.find(\"div\", class_=\"cts_atclst\")\n",
    "    if div_tag:\n",
    "        articles = div_tag.find_all(\"li\")  # <li> 태그 리스트 반환\n",
    "        for article in articles:\n",
    "            try:\n",
    "                # 제목 추출\n",
    "                title_tag = article.find(\"span\", class_=\"tt2\")\n",
    "                title = title_tag.get_text(strip=True) if title_tag else \"제목 없음\"\n",
    "                titles.append(title)\n",
    "\n",
    "                # 링크 추출\n",
    "                link_tag = article.find(\"a\")\n",
    "                link = link_tag[\"href\"] if link_tag and link_tag.has_attr(\"href\") else \"링크 없음\"\n",
    "                links.append(link)\n",
    "\n",
    "                # 작성일자 추출\n",
    "                date_tag = article.find(\"span\", class_=\"pbdt\")\n",
    "                date = date_tag.get_text(strip=True) if date_tag else \"날짜 없음\"\n",
    "                dates.append(date)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"개별 기사 처리 중 오류 발생: {e}\")\n",
    "#--------------------------------------------------------------------------------------------------------------\n",
    "# 데이터 저장\n",
    "try:\n",
    "    df = pd.DataFrame({\"제목\": titles, \"링크\": links, \"작성일자\": dates})\n",
    "    df.to_csv(\"이재명_연합뉴스_기사목록.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    print(\"데이터가 CSV 파일로 저장되었습니다.\")\n",
    "except Exception as e:\n",
    "    print(\"CSV 저장 중 오류 발생:\", e)\n",
    "\n",
    "# 브라우저 종료\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e34faaaf-fd69-445e-ae97-de7e51503a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터가 CSV 파일로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "## 이거 하나만\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Chrome 옵션 설정\n",
    "chrome_options = Options()\n",
    "chrome_options.add_experimental_option(\"detach\", True)\n",
    "chrome_options.add_argument(\"--disable-notifications\")  # 알림 비활성화\n",
    "chrome_options.add_argument(\"--start-maximized\")  # 브라우저 최대화\n",
    "chrome_options.add_argument(\"--disable-popup-blocking\")  # 팝업 차단 비활성화\n",
    "\n",
    "# ChromeDriver 경로 설정 및 Service 객체 생성\n",
    "s = Service(r\"C:\\Users\\junse\\2024-2\\python_onlinedata_analysis\\chromedriver-win64\\chromedriver.exe\")  # ChromeDriver 실행 파일 경로\n",
    "driver = webdriver.Chrome(service=s, options=chrome_options)  # WebDriver 객체 생성\n",
    "titles, links, dates = [], [], [] # 제목, 링크, 작성일자를 저장할 리스트 생성\n",
    "\n",
    "# 연합뉴스 사이트 접속\n",
    "for numb in range(1,21):\n",
    "    url_Y = f\"https://www.yna.co.kr/search/index?query=%EC%9D%B4%EC%9E%AC%EB%AA%85&ctype=A&sort=weight&from=20220215&to=20220308&period=diy&page_no={numb}\"\n",
    "    driver.get(url_Y)\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "    # 기사 데이터 크롤링\n",
    "    time.sleep(5)\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, \"lxml\")  # BeautifulSoup으로 HTML 파싱\n",
    "\n",
    "    # <div class=\"cts_atclst\"> 내부의 모든 <li> 태그 찾기\n",
    "    div_tag = soup.find(\"div\", class_=\"cts_atclst\")\n",
    "    if div_tag:\n",
    "        articles = div_tag.find_all(\"li\")  # <li> 태그 리스트 반환\n",
    "        for article in articles:\n",
    "            try:\n",
    "                # 제목 추출\n",
    "                title_tag = article.find(\"span\", class_=\"tt2\")\n",
    "                title = title_tag.get_text(strip=True) if title_tag else \"제목 없음\"\n",
    "                titles.append(title)\n",
    "\n",
    "                # 링크 추출\n",
    "                link_tag = article.find(\"a\")\n",
    "                link = link_tag[\"href\"] if link_tag and link_tag.has_attr(\"href\") else \"링크 없음\"\n",
    "                links.append(link)\n",
    "\n",
    "                # 작성일자 추출\n",
    "                date_tag = article.find(\"span\", class_=\"pbdt\")\n",
    "                date = date_tag.get_text(strip=True) if date_tag else \"날짜 없음\"\n",
    "                dates.append(date)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"개별 기사 처리 중 오류 발생: {e}\")\n",
    "#--------------------------------------------------------------------------------------------------------------\n",
    "# 데이터 저장\n",
    "try:\n",
    "    df = pd.DataFrame({\"제목\": titles, \"링크\": links, \"작성일자\": dates})\n",
    "    df.to_csv(\"이재명_연합뉴스_기사목록200.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    print(\"데이터가 CSV 파일로 저장되었습니다.\")\n",
    "except Exception as e:\n",
    "    print(\"CSV 저장 중 오류 발생:\", e)\n",
    "\n",
    "# 브라우저 종료\n",
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
