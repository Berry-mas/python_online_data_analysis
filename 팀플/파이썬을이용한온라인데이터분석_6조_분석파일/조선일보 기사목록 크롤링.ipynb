{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b58af2cc-9a72-4bed-b069-fb77c4c5b895",
   "metadata": {},
   "source": [
    "# 1. 윤석열 스크랩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cba189a9-40ec-420d-bc7a-fe0bdc9af461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터가 CSV 파일로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 첫번째 페이지\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Chrome 옵션 설정\n",
    "chrome_options = Options()\n",
    "chrome_options.add_experimental_option(\"detach\", True)\n",
    "chrome_options.add_argument(\"--disable-notifications\")  # 알림 비활성화\n",
    "chrome_options.add_argument(\"--start-maximized\")  # 브라우저 최대화\n",
    "chrome_options.add_argument(\"--disable-popup-blocking\")  # 팝업 차단 비활성화\n",
    "\n",
    "# ChromeDriver 경로 설정 및 Service 객체 생성\n",
    "s = Service(r\"C:\\Users\\junse\\2024-2\\python_onlinedata_analysis\\chromedriver-win64\\chromedriver.exe\")  # ChromeDriver 실행 파일 경로\n",
    "driver = webdriver.Chrome(service=s, options=chrome_options)  # WebDriver 객체 생성\n",
    "\n",
    "# 조선일보 사이트 접속\n",
    "url_Ch = \"https://www.chosun.com/\"\n",
    "driver.get(url_Ch)\n",
    "\n",
    "try:\n",
    "    # 돋보기\n",
    "    search_icon =  driver.find_element(By.XPATH, '//*[@id=\"nav-bar-left\"]/div[2]')\n",
    "    search_icon.click()\n",
    "    \n",
    "    # 검색창 대기\n",
    "    search_input = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.XPATH, '//*[@id=\"nav-bar-left\"]/div[2]/form/div/input'))\n",
    "    )\n",
    "    # 검색어 입력\n",
    "    search_input.send_keys(\"윤석열\")\n",
    "    search_input.send_keys(Keys.RETURN)  # 엔터키로 검색\n",
    "\n",
    "    # 다음 페이지 로딩 대기\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.XPATH, '//*[@id=\"main\"]/div[1]/div[1]/div[1]/span'))\n",
    "    )\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"검색창에 접근할 수 없습니다:\", e) \n",
    "    driver.quit()\n",
    "    exit()\n",
    "    \n",
    "# 검색 기간 설정\n",
    "try:\n",
    "    relevance = driver.find_element(By.XPATH, '//*[@id=\"main\"]/div[1]/div[1]/div[2]/div[1]/span')\n",
    "    relevance.click()\n",
    "    \n",
    "    period = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, '//*[@id=\"main\"]/div[1]/div[2]/div/div[1]/button/span'))\n",
    "    )\n",
    "    period.click()           # 기간 버튼 누르기\n",
    "\n",
    "    direct_input = driver.find_element(By.XPATH, '//*[@id=\"direct\"]/span')   ## 직접입력 누르기\n",
    "    direct_input.click()\n",
    "\n",
    "    ## 시작 시점\n",
    "    year_start  = driver.find_element(By.XPATH, '//*[@id=\"year_2022\"]')   ## 연도 누르기 (2022)\n",
    "    year_start.click()   \n",
    "\n",
    "    month_start  = driver.find_element(By.XPATH, '//*[@id=\"main\"]/div[1]/div[2]/div/div[1]/div/div[3]/div[1]/div[3]/ul[2]/li[2]')   ## 달 누르기 (02)\n",
    "    month_start.click()\n",
    "    \n",
    "    day_start = driver.find_element(By.XPATH, '//*[@id=\"main\"]/div[1]/div[2]/div/div[1]/div/div[3]/div[1]/div[3]/ul[3]/li[15]')   ## 일 누르기 (15)\n",
    "    day_start.click()  \n",
    "    \n",
    "    ## 끝 시점\n",
    "    end_button = driver.find_element(By.XPATH, '//*[@id=\"end\"]')   ## 끝 시점 창 누르기\n",
    "    end_button.click()\n",
    "    \n",
    "    year_end  = driver.find_element(By.XPATH, '//*[@id=\"year_2022\"]')   ## 연도 누르기 (2022)\n",
    "    year_end.click()   \n",
    "\n",
    "    month_end = driver.find_element(By.XPATH, '//*[@id=\"main\"]/div[1]/div[2]/div/div[1]/div/div[3]/div[1]/div[3]/ul[2]/li[3]')   ## 달 누르기 (03)\n",
    "    month_end.click()\n",
    "    \n",
    "    day_end = driver.find_element(By.XPATH, '//*[@id=\"main\"]/div[1]/div[2]/div/div[1]/div/div[3]/div[1]/div[3]/ul[3]/li[8]')   ## 일 누르기 (08)\n",
    "    day_end.click()  \n",
    "    \n",
    "    # 적용 누르기\n",
    "    apply_icon = driver.find_element(By.XPATH, '//*[@id=\"main\"]/div[1]/div[2]/div/div[1]/div/div[3]/div[2]/button[1]')\n",
    "    apply_icon.click()\n",
    "\n",
    "    # 옵션 유지 누르기 \n",
    "    keep_option = driver.find_element(By.XPATH, '//*[@id=\"main\"]/div[3]/div/label')\n",
    "    keep_option.click()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"검색 기간을 설정할 수 없습니다:\", e)\n",
    "    driver.quit()\n",
    "    exit()\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "# 기사 데이터 크롤링\n",
    "time.sleep(5)\n",
    "titles, links, dates = [], [], [] # 제목, 링크, 작성일자를 저장할 리스트 생성\n",
    "page_source = driver.page_source\n",
    "soup = BeautifulSoup(page_source, \"lxml\")  # BeautifulSoup으로 HTML 파싱\n",
    "\n",
    "articles = soup.find_all(\"div\", class_=\"story-card__headline\")\n",
    "for article in articles:\n",
    "        \n",
    "    try:\n",
    "        # 기사 제목\n",
    "        title_tag = article.find(\"a\", class_=\"text__link\")\n",
    "        if title_tag :\n",
    "            titles.append(title_tag.get_text(strip=True))\n",
    "            links.append(title_tag[\"href\"])\n",
    "        else:\n",
    "            titles.append(\"제목없음\")\n",
    "            links.append(\"링크없음\")\n",
    "    except Exception as e: \n",
    "        print(f\"헤드라인 데이터 추출 오류: {e}\")\n",
    "        \n",
    "breadcrumb_articles = soup.find_all(\"div\", class_=\"story-card__breadcrumb\")\n",
    "for breadcrumb in breadcrumb_articles:\n",
    "    try:\n",
    "        # 날짜 추출\n",
    "        date_tag = breadcrumb.find_all(\"span\")[-1]  # 마지막 <span> 태그에서 날짜 추출\n",
    "        if date_tag:\n",
    "            dates.append(date_tag.get_text(strip=True))\n",
    "        else:\n",
    "            dates.append(\"날짜 없음\")\n",
    "    except Exception as e:\n",
    "        print(f\"날짜 데이터 추출 오류: {e}\")\n",
    "    \n",
    "#--------------------------------------------------------------------------------------------------------------\n",
    "# 데이터 저장\n",
    "try:\n",
    "    df = pd.DataFrame({\"제목\": titles, \"링크\": links, \"작성일자\": dates})\n",
    "    df = df.drop_duplicates()\n",
    "    df.to_csv(\"윤석열_조선일보_기사목록.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    print(\"데이터가 CSV 파일로 저장되었습니다.\")\n",
    "except Exception as e:\n",
    "    print(\"CSV 저장 중 오류 발생:\", e)\n",
    "\n",
    "# 브라우저 종료\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "991e1641-b9e6-481b-87ec-faf48de293a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터가 CSV 파일로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "## 이거 반복\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Chrome 옵션 설정\n",
    "chrome_options = Options()\n",
    "chrome_options.add_experimental_option(\"detach\", True)\n",
    "chrome_options.add_argument(\"--disable-notifications\")  # 알림 비활성화\n",
    "chrome_options.add_argument(\"--start-maximized\")  # 브라우저 최대화\n",
    "chrome_options.add_argument(\"--disable-popup-blocking\")  # 팝업 차단 비활성화\n",
    "\n",
    "# ChromeDriver 경로 설정 및 Service 객체 생성\n",
    "s = Service(r\"C:\\Users\\junse\\2024-2\\python_onlinedata_analysis\\chromedriver-win64\\chromedriver.exe\")  # ChromeDriver 실행 파일 경로\n",
    "driver = webdriver.Chrome(service=s, options=chrome_options)  # WebDriver 객체 생성\n",
    "\n",
    "# 조선일보 사이트 접속\n",
    "for numb in range(2,51):\n",
    "    url_Ch = f\"https://www.chosun.com/nsearch/?query=%EC%9C%A4%EC%84%9D%EC%97%B4&page={numb}&siteid=&sort=0&date_period=direct&date_start=20220215&date_end=20220308&writer=&field=&emd_word=&expt_word=&opt_chk=true&app_check=0&website=www,chosun&category=\"\n",
    "    driver.get(url_Ch)\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # 기사 데이터 크롤링\n",
    "    time.sleep(5)\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, \"lxml\")  # BeautifulSoup으로 HTML 파싱\n",
    "\n",
    "    articles = soup.find_all(\"div\", class_=\"story-card__headline\")\n",
    "    for article in articles:\n",
    "        \n",
    "        try:\n",
    "            # 기사 제목, 링크\n",
    "            title_tag = article.find(\"a\", class_=\"text__link\")\n",
    "            if title_tag :\n",
    "                titles.append(title_tag.get_text(strip=True))\n",
    "                links.append(title_tag[\"href\"])\n",
    "            else:\n",
    "                titles.append(\"제목없음\")\n",
    "                links.append(\"링크없음\")\n",
    "        except Exception as e: \n",
    "            print(f\"헤드라인 데이터 추출 오류: {e}\")\n",
    "        \n",
    "    breadcrumb_articles = soup.find_all(\"div\", class_=\"story-card__breadcrumb\")\n",
    "    for breadcrumb in breadcrumb_articles:\n",
    "        try:\n",
    "            # 날짜 추출\n",
    "            date_tag = breadcrumb.find_all(\"span\")[-1]  # 마지막 <span> 태그에서 날짜 추출\n",
    "            if date_tag:\n",
    "                dates.append(date_tag.get_text(strip=True))\n",
    "            else:\n",
    "                dates.append(\"날짜 없음\")\n",
    "        except Exception as e:\n",
    "            print(f\"날짜 데이터 추출 오류: {e}\")\n",
    "    \n",
    "#--------------------------------------------------------------------------------------------------------------\n",
    "# 데이터 저장\n",
    "try:\n",
    "    df = pd.DataFrame({\"제목\": titles, \"링크\": links, \"작성일자\": dates})\n",
    "    df = df.drop_duplicates()\n",
    "    df.to_csv(\"윤석열_조선일보_기사목록.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    print(\"데이터가 CSV 파일로 저장되었습니다.\")\n",
    "except Exception as e:\n",
    "    print(\"CSV 저장 중 오류 발생:\", e)\n",
    "\n",
    "# 브라우저 종료\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a17d5617-7c5d-4bd5-b379-891525a3d51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터가 CSV 파일로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "## 이거 하나만\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Chrome 옵션 설정\n",
    "chrome_options = Options()\n",
    "chrome_options.add_experimental_option(\"detach\", True)\n",
    "chrome_options.add_argument(\"--disable-notifications\")  # 알림 비활성화\n",
    "chrome_options.add_argument(\"--start-maximized\")  # 브라우저 최대화\n",
    "chrome_options.add_argument(\"--disable-popup-blocking\")  # 팝업 차단 비활성화\n",
    "\n",
    "# ChromeDriver 경로 설정 및 Service 객체 생성\n",
    "s = Service(r\"C:\\Users\\junse\\2024-2\\python_onlinedata_analysis\\chromedriver-win64\\chromedriver.exe\")  # ChromeDriver 실행 파일 경로\n",
    "driver = webdriver.Chrome(service=s, options=chrome_options)  # WebDriver 객체 생성\n",
    "titles, links, dates = [], [], [] # 제목, 링크, 작성일자를 저장할 리스트 생성\n",
    "\n",
    "# 조선일보 사이트 접속\n",
    "for numb in range(1,21):\n",
    "    url_Ch = f\"https://www.chosun.com/nsearch/?query=%EC%9C%A4%EC%84%9D%EC%97%B4&page={numb}&siteid=&sort=0&date_period=direct&date_start=20220215&date_end=20220308&writer=&field=&emd_word=&expt_word=&opt_chk=true&app_check=0&website=www,chosun&category=\"\n",
    "    driver.get(url_Ch)\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # 기사 데이터 크롤링\n",
    "    time.sleep(3)\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, \"lxml\")  # BeautifulSoup으로 HTML 파싱\n",
    "\n",
    "    articles = soup.find_all(\"div\", class_=\"story-card__headline\")\n",
    "    for article in articles:\n",
    "        \n",
    "        try:\n",
    "            # 기사 제목, 링크\n",
    "            title_tag = article.find(\"a\", class_=\"text__link\")\n",
    "            if title_tag :\n",
    "                titles.append(title_tag.get_text(strip=True))\n",
    "                links.append(title_tag[\"href\"])\n",
    "            else:\n",
    "                titles.append(\"제목없음\")\n",
    "                links.append(\"링크없음\")\n",
    "        except Exception as e: \n",
    "            print(f\"헤드라인 데이터 추출 오류: {e}\")\n",
    "        \n",
    "    breadcrumb_articles = soup.find_all(\"div\", class_=\"story-card__breadcrumb\")\n",
    "    for breadcrumb in breadcrumb_articles:\n",
    "        try:\n",
    "            # 날짜 추출\n",
    "            date_tag = breadcrumb.find_all(\"span\")[-1]  # 마지막 <span> 태그에서 날짜 추출\n",
    "            if date_tag:\n",
    "                dates.append(date_tag.get_text(strip=True))\n",
    "            else:\n",
    "                dates.append(\"날짜 없음\")\n",
    "        except Exception as e:\n",
    "            print(f\"날짜 데이터 추출 오류: {e}\")\n",
    "    \n",
    "#--------------------------------------------------------------------------------------------------------------\n",
    "# 데이터 저장\n",
    "try:\n",
    "    df = pd.DataFrame({\"제목\": titles, \"링크\": links, \"작성일자\": dates})\n",
    "    df = df.drop_duplicates()\n",
    "    df.to_csv(\"윤석열_조선일보_기사목록200.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    print(\"데이터가 CSV 파일로 저장되었습니다.\")\n",
    "except Exception as e:\n",
    "    print(\"CSV 저장 중 오류 발생:\", e)\n",
    "\n",
    "# 브라우저 종료\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103a72fb-a117-42a7-9238-bfa3a12f595f",
   "metadata": {},
   "source": [
    "# 2. 이재명 스크랩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1dc7b61b-38fa-49aa-9018-ef59c9269f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터가 CSV 파일로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 첫번째 페이지\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Chrome 옵션 설정\n",
    "chrome_options = Options()\n",
    "chrome_options.add_experimental_option(\"detach\", True)\n",
    "chrome_options.add_argument(\"--disable-notifications\")  # 알림 비활성화\n",
    "chrome_options.add_argument(\"--start-maximized\")  # 브라우저 최대화\n",
    "chrome_options.add_argument(\"--disable-popup-blocking\")  # 팝업 차단 비활성화\n",
    "\n",
    "# ChromeDriver 경로 설정 및 Service 객체 생성\n",
    "s = Service(r\"C:\\Users\\junse\\2024-2\\python_onlinedata_analysis\\chromedriver-win64\\chromedriver.exe\")  # ChromeDriver 실행 파일 경로\n",
    "driver = webdriver.Chrome(service=s, options=chrome_options)  # WebDriver 객체 생성\n",
    "\n",
    "# 조선일보 사이트 접속\n",
    "url_Ch = \"https://www.chosun.com/\"\n",
    "driver.get(url_Ch)\n",
    "\n",
    "try:\n",
    "    # 돋보기\n",
    "    search_icon =  driver.find_element(By.XPATH, '//*[@id=\"nav-bar-left\"]/div[2]')\n",
    "    search_icon.click()\n",
    "    \n",
    "    # 검색창 대기\n",
    "    search_input = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.XPATH, '//*[@id=\"nav-bar-left\"]/div[2]/form/div/input'))\n",
    "    )\n",
    "    # 검색어 입력\n",
    "    search_input.send_keys(\"이재명\")\n",
    "    search_input.send_keys(Keys.RETURN)  # 엔터키로 검색\n",
    "\n",
    "    # 다음 페이지 로딩 대기\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.XPATH, '//*[@id=\"main\"]/div[1]/div[1]/div[1]/span'))\n",
    "    )\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"검색창에 접근할 수 없습니다:\", e) \n",
    "    driver.quit()\n",
    "    exit()\n",
    "    \n",
    "# 검색 기간 설정\n",
    "try:\n",
    "    relevance = driver.find_element(By.XPATH, '//*[@id=\"main\"]/div[1]/div[1]/div[2]/div[1]/span')\n",
    "    relevance.click()\n",
    "    \n",
    "    period = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, '//*[@id=\"main\"]/div[1]/div[2]/div/div[1]/button/span'))\n",
    "    )\n",
    "    period.click()           # 기간 버튼 누르기\n",
    "\n",
    "    direct_input = driver.find_element(By.XPATH, '//*[@id=\"direct\"]/span')   ## 직접입력 누르기\n",
    "    direct_input.click()\n",
    "\n",
    "    ## 시작 시점\n",
    "    year_start  = driver.find_element(By.XPATH, '//*[@id=\"year_2022\"]')   ## 연도 누르기 (2022)\n",
    "    year_start.click()   \n",
    "\n",
    "    month_start  = driver.find_element(By.XPATH, '//*[@id=\"main\"]/div[1]/div[2]/div/div[1]/div/div[3]/div[1]/div[3]/ul[2]/li[2]')   ## 달 누르기 (02)\n",
    "    month_start.click()\n",
    "    \n",
    "    day_start = driver.find_element(By.XPATH, '//*[@id=\"main\"]/div[1]/div[2]/div/div[1]/div/div[3]/div[1]/div[3]/ul[3]/li[15]')   ## 일 누르기 (15)\n",
    "    day_start.click()  \n",
    "    \n",
    "    ## 끝 시점\n",
    "    end_button = driver.find_element(By.XPATH, '//*[@id=\"end\"]')   ## 끝 시점 창 누르기\n",
    "    end_button.click()\n",
    "    \n",
    "    year_end  = driver.find_element(By.XPATH, '//*[@id=\"year_2022\"]')   ## 연도 누르기 (2022)\n",
    "    year_end.click()   \n",
    "\n",
    "    month_end = driver.find_element(By.XPATH, '//*[@id=\"main\"]/div[1]/div[2]/div/div[1]/div/div[3]/div[1]/div[3]/ul[2]/li[3]')   ## 달 누르기 (03)\n",
    "    month_end.click()\n",
    "    \n",
    "    day_end = driver.find_element(By.XPATH, '//*[@id=\"main\"]/div[1]/div[2]/div/div[1]/div/div[3]/div[1]/div[3]/ul[3]/li[8]')   ## 일 누르기 (08)\n",
    "    day_end.click()  \n",
    "    \n",
    "    # 적용 누르기\n",
    "    apply_icon = driver.find_element(By.XPATH, '//*[@id=\"main\"]/div[1]/div[2]/div/div[1]/div/div[3]/div[2]/button[1]')\n",
    "    apply_icon.click()\n",
    "\n",
    "    # 옵션 유지 누르기 \n",
    "    keep_option = driver.find_element(By.XPATH, '//*[@id=\"main\"]/div[3]/div/label')\n",
    "    keep_option.click()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"검색 기간을 설정할 수 없습니다:\", e)\n",
    "    driver.quit()\n",
    "    exit()\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "# 기사 데이터 크롤링\n",
    "time.sleep(5)\n",
    "titles, links, dates = [], [], [] # 제목, 링크, 작성일자를 저장할 리스트 생성\n",
    "page_source = driver.page_source\n",
    "soup = BeautifulSoup(page_source, \"lxml\")  # BeautifulSoup으로 HTML 파싱\n",
    "\n",
    "articles = soup.find_all(\"div\", class_=\"story-card__headline\")\n",
    "for article in articles:\n",
    "        \n",
    "    try:\n",
    "        # 기사 제목\n",
    "        title_tag = article.find(\"a\", class_=\"text__link\")\n",
    "        if title_tag :\n",
    "            titles.append(title_tag.get_text(strip=True))\n",
    "            links.append(title_tag[\"href\"])\n",
    "        else:\n",
    "            titles.append(\"제목없음\")\n",
    "            links.append(\"링크없음\")\n",
    "    except Exception as e: \n",
    "        print(f\"헤드라인 데이터 추출 오류: {e}\")\n",
    "        \n",
    "breadcrumb_articles = soup.find_all(\"div\", class_=\"story-card__breadcrumb\")\n",
    "for breadcrumb in breadcrumb_articles:\n",
    "    try:\n",
    "        # 날짜 추출\n",
    "        date_tag = breadcrumb.find_all(\"span\")[-1]  # 마지막 <span> 태그에서 날짜 추출\n",
    "        if date_tag:\n",
    "            dates.append(date_tag.get_text(strip=True))\n",
    "        else:\n",
    "            dates.append(\"날짜 없음\")\n",
    "    except Exception as e:\n",
    "        print(f\"날짜 데이터 추출 오류: {e}\")\n",
    "    \n",
    "#--------------------------------------------------------------------------------------------------------------\n",
    "# 데이터 저장\n",
    "try:\n",
    "    df = pd.DataFrame({\"제목\": titles, \"링크\": links, \"작성일자\": dates})\n",
    "    df = df.drop_duplicates()\n",
    "    df.to_csv(\"이재명_조선일보_기사목록.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    print(\"데이터가 CSV 파일로 저장되었습니다.\")\n",
    "except Exception as e:\n",
    "    print(\"CSV 저장 중 오류 발생:\", e)\n",
    "\n",
    "# 브라우저 종료\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0daeef4b-87d3-4169-89da-8f801954c796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터가 CSV 파일로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "## 이거 반복\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Chrome 옵션 설정\n",
    "chrome_options = Options()\n",
    "chrome_options.add_experimental_option(\"detach\", True)\n",
    "chrome_options.add_argument(\"--disable-notifications\")  # 알림 비활성화\n",
    "chrome_options.add_argument(\"--start-maximized\")  # 브라우저 최대화\n",
    "chrome_options.add_argument(\"--disable-popup-blocking\")  # 팝업 차단 비활성화\n",
    "\n",
    "# ChromeDriver 경로 설정 및 Service 객체 생성\n",
    "s = Service(r\"C:\\Users\\junse\\2024-2\\python_onlinedata_analysis\\chromedriver-win64\\chromedriver.exe\")  # ChromeDriver 실행 파일 경로\n",
    "driver = webdriver.Chrome(service=s, options=chrome_options)  # WebDriver 객체 생성\n",
    "\n",
    "# 조선일보 사이트 접속\n",
    "for numb in range(2,51):\n",
    "    url_Ch = f\"https://www.chosun.com/nsearch/?query=%EC%9D%B4%EC%9E%AC%EB%AA%85&page={numb}&siteid=&sort=0&date_period=direct&date_start=20220215&date_end=20220308&writer=&field=&emd_word=&expt_word=&opt_chk=true&app_check=0&website=www,chosun&category=\"\n",
    "    driver.get(url_Ch)\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # 기사 데이터 크롤링\n",
    "    time.sleep(5)\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, \"lxml\")  # BeautifulSoup으로 HTML 파싱\n",
    "\n",
    "    articles = soup.find_all(\"div\", class_=\"story-card__headline\")\n",
    "    for article in articles:\n",
    "        \n",
    "        try:\n",
    "            # 기사 제목, 링크\n",
    "            title_tag = article.find(\"a\", class_=\"text__link\")\n",
    "            if title_tag :\n",
    "                titles.append(title_tag.get_text(strip=True))\n",
    "                links.append(title_tag[\"href\"])\n",
    "            else:\n",
    "                titles.append(\"제목없음\")\n",
    "                links.append(\"링크없음\")\n",
    "        except Exception as e: \n",
    "            print(f\"헤드라인 데이터 추출 오류: {e}\")\n",
    "        \n",
    "    breadcrumb_articles = soup.find_all(\"div\", class_=\"story-card__breadcrumb\")\n",
    "    for breadcrumb in breadcrumb_articles:\n",
    "        try:\n",
    "            # 날짜 추출\n",
    "            date_tag = breadcrumb.find_all(\"span\")[-1]  # 마지막 <span> 태그에서 날짜 추출\n",
    "            if date_tag:\n",
    "                dates.append(date_tag.get_text(strip=True))\n",
    "            else:\n",
    "                dates.append(\"날짜 없음\")\n",
    "        except Exception as e:\n",
    "            print(f\"날짜 데이터 추출 오류: {e}\")\n",
    "    \n",
    "#--------------------------------------------------------------------------------------------------------------\n",
    "# 데이터 저장\n",
    "try:\n",
    "    df = pd.DataFrame({\"제목\": titles, \"링크\": links, \"작성일자\": dates})\n",
    "    df = df.drop_duplicates()\n",
    "    df.to_csv(\"이재명_조선일보_기사목록.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    print(\"데이터가 CSV 파일로 저장되었습니다.\")\n",
    "except Exception as e:\n",
    "    print(\"CSV 저장 중 오류 발생:\", e)\n",
    "\n",
    "# 브라우저 종료\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20da0c88-0f11-4d06-8da6-374e0689f76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터가 CSV 파일로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "## 이거 하나만\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Chrome 옵션 설정\n",
    "chrome_options = Options()\n",
    "chrome_options.add_experimental_option(\"detach\", True)\n",
    "chrome_options.add_argument(\"--disable-notifications\")  # 알림 비활성화\n",
    "chrome_options.add_argument(\"--start-maximized\")  # 브라우저 최대화\n",
    "chrome_options.add_argument(\"--disable-popup-blocking\")  # 팝업 차단 비활성화\n",
    "\n",
    "# ChromeDriver 경로 설정 및 Service 객체 생성\n",
    "s = Service(r\"C:\\Users\\junse\\2024-2\\python_onlinedata_analysis\\chromedriver-win64\\chromedriver.exe\")  # ChromeDriver 실행 파일 경로\n",
    "driver = webdriver.Chrome(service=s, options=chrome_options)  # WebDriver 객체 생성\n",
    "titles, links, dates = [], [], [] # 제목, 링크, 작성일자를 저장할 리스트 생성\n",
    "\n",
    "# 조선일보 사이트 접속\n",
    "for numb in range(1,21):\n",
    "    url_Ch = f\"https://www.chosun.com/nsearch/?query=%EC%9D%B4%EC%9E%AC%EB%AA%85&page={numb}&siteid=&sort=0&date_period=direct&date_start=20220215&date_end=20220308&writer=&field=&emd_word=&expt_word=&opt_chk=true&app_check=0&website=www,chosun&category=\"\n",
    "    driver.get(url_Ch)\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # 기사 데이터 크롤링\n",
    "    time.sleep(3)\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, \"lxml\")  # BeautifulSoup으로 HTML 파싱\n",
    "\n",
    "    articles = soup.find_all(\"div\", class_=\"story-card__headline\")\n",
    "    for article in articles:\n",
    "        \n",
    "        try:\n",
    "            # 기사 제목, 링크\n",
    "            title_tag = article.find(\"a\", class_=\"text__link\")\n",
    "            if title_tag :\n",
    "                titles.append(title_tag.get_text(strip=True))\n",
    "                links.append(title_tag[\"href\"])\n",
    "            else:\n",
    "                titles.append(\"제목없음\")\n",
    "                links.append(\"링크없음\")\n",
    "        except Exception as e: \n",
    "            print(f\"헤드라인 데이터 추출 오류: {e}\")\n",
    "        \n",
    "    breadcrumb_articles = soup.find_all(\"div\", class_=\"story-card__breadcrumb\")\n",
    "    for breadcrumb in breadcrumb_articles:\n",
    "        try:\n",
    "            # 날짜 추출\n",
    "            date_tag = breadcrumb.find_all(\"span\")[-1]  # 마지막 <span> 태그에서 날짜 추출\n",
    "            if date_tag:\n",
    "                dates.append(date_tag.get_text(strip=True))\n",
    "            else:\n",
    "                dates.append(\"날짜 없음\")\n",
    "        except Exception as e:\n",
    "            print(f\"날짜 데이터 추출 오류: {e}\")\n",
    "    \n",
    "#--------------------------------------------------------------------------------------------------------------\n",
    "# 데이터 저장\n",
    "try:\n",
    "    df = pd.DataFrame({\"제목\": titles, \"링크\": links, \"작성일자\": dates})\n",
    "    df = df.drop_duplicates()\n",
    "    df.to_csv(\"이재명_조선일보_기사목록200.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    print(\"데이터가 CSV 파일로 저장되었습니다.\")\n",
    "except Exception as e:\n",
    "    print(\"CSV 저장 중 오류 발생:\", e)\n",
    "\n",
    "# 브라우저 종료\n",
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
