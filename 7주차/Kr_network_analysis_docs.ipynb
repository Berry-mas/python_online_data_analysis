{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전처리 하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#여러 문서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 코드는 그냥 가져다 쓰자\n",
    "\n",
    "from kiwipiepy import Kiwi\n",
    "import re\n",
    "from collections import Counter\n",
    "kiwi = Kiwi()\n",
    "from kiwipiepy.utils import Stopwords\n",
    "stopwords = Stopwords()\n",
    "customized_stopwords = ['연합뉴스', '네이버', '기자', '문의']\n",
    "for word in customized_stopwords:\n",
    "    stopwords.add((word, 'NNG'))\n",
    "\n",
    "def do_Kr_preprocessing(text):\n",
    "    cleaned_content = re.sub(r'[^\\w\\d\\s]','',text) # To remove symbols\n",
    "    cleaned_content = cleaned_content.lower() # Case conversion, upper -> lower\n",
    "    word_tokens = kiwi.tokenize(cleaned_content, stopwords=stopwords) \n",
    "    NN_words = []   # To select nouns\n",
    "    for token in word_tokens:\n",
    "        if 'NN' in token.tag:\n",
    "            NN_words.append(token.form)\n",
    "    final_NN_words = []\n",
    "    for word in NN_words:\n",
    "        if len(word) > 1:\n",
    "            final_NN_words.append(word)\n",
    "    \n",
    "    return final_NN_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네트워크 생성\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "def construct_network(text):\n",
    "    vectorizer = CountVectorizer(min_df=1, ngram_range=(1,1))\n",
    "    DTM_tf = vectorizer.fit_transform(text)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    DTM = np.array(DTM_tf.todense())\n",
    "    DTM_binary = np.sign(DTM)\n",
    "    words_cooccurrence = np.dot(DTM_binary.T, DTM_binary)\n",
    "    np.fill_diagonal(words_cooccurrence, 0)\n",
    "    #print(feature_names) # 네트워크를 구성하는 단어 출력하기\n",
    "    #print(words_cooccurrence) # 단어들 간의 인접행렬 출력하기\n",
    "    g = nx.convert_matrix.from_numpy_array(words_cooccurrence)\n",
    "    mapping = {}\n",
    "    for k, word in enumerate(feature_names):\n",
    "        mapping[k]=word\n",
    "    g1 = nx.relabel_nodes(g, mapping)\n",
    "    return g1\n",
    "\n",
    "def get_text_network(text, selected_words):        \n",
    "    cleaned_docs = [do_Kr_preprocessing(doc) for doc in text]\n",
    "    new_docs=[]\n",
    "    for doc in cleaned_docs:\n",
    "        new_doc=[]\n",
    "        for word in doc:\n",
    "            if word in selected_words:\n",
    "                new_doc.append(word)\n",
    "        if len(new_doc)>1:\n",
    "            new_docs.append(' '.join(new_doc))\n",
    "    return construct_network(new_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text data 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "mypath = './example_Kr/' # 네트워크 분석을 하고자하는 문서들이 저장되어 있는 폴더\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "onlyfiles.sort()\n",
    "\n",
    "total_docs = []\n",
    "for file in onlyfiles:\n",
    "    file_path = mypath+file\n",
    "    with open(file_path, 'r', encoding='utf8') as f:\n",
    "        content = f.read()\n",
    "    total_docs.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['사과 과일 여름 바나나 딸기.\\n딸기 사과 바지 옷 하늘 구름.',\n",
       " '우리나라 대한민국 일본 중국 미국.\\n대한민국 북한 일본 중국.',\n",
       " '대한민국 미국 축구 북한 농구.\\n축구 북한 대한민국 야구.\\n가을 축구 대한민국 손흥민.',\n",
       " '바나나 사과 주스 건강 매일.\\n키위 바나나 주스 아침.\\n건강 달리기 걷기 과일.',\n",
       " '가을 자전거 하늘 걷기.\\n도로 주행 여행 드라이브.\\n겨울 눈 추위 감기.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_nouns = ['사과', '바나나', '주스', '축구', '대한민국']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text network 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = get_text_network(total_docs, final_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AtlasView({'사과': {'weight': 2}, '주스': {'weight': 1}})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g['바나나']   #바나나와 사과가 함께 쓰인 문서는 2개이고, 바나나와 주스가 함께 쓰인 문서는 1개임을 알 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graphml 파일로 저장하기\n",
    "nx.write_graphml(g, 'Kr_test_docs.graphml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#한글과 영어 시각화 차이\n",
    "# gephi에서 영어 폰트가 기본 선택임 -> 한글 폰트로 바꾸면 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
